{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Template\n",
    "Importing librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our dataset but we need to distinguish the matrix of features and the dependent variable vector.\n",
    "\n",
    "So we are going to create a matrix of features : the matrix of the 3 independent variables (Country, Age, Salary) \n",
    "\n",
    "We are going to put in a variable X all the observations (3 columns):\n",
    "- iloc[:, :-1] will take every columns except the last one -> __iloc[lines, columns]__ and __\":\"__ represent a range. \n",
    "- values method will transform our dataframe to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create the dependent variable vector \"Y\" only get the last column (index = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = dataset.iloc[:, 3].values\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data \n",
    "If we look at our dataset we see that we have missing data (NaN values).\n",
    "\n",
    "The first idea to handle this is to replace missing data by the __mean__ of all the values in each column.\n",
    "\n",
    "To do this we are going to use the __Imputer__ class of the __scikit learn__ library: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Creating the object\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "\n",
    "# Fitting our imputer with columns in the 1:3 (upper bound excluded) range (Age, Salary) -> where there is missing data\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "\n",
    "# Apply our imputer to the matrix\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data\n",
    "How to encode categorical data.\n",
    "\n",
    "In our dataset we have to categorical variables : Country and Purchased (they contain categories)\n",
    "\n",
    "The problem is that we can't keep __text__ variables in the equations because we only want numbers. So we need to encode them into __numbers__.\n",
    "\n",
    "To do this we are going to use the __LabelEncoder__ class of the __scikit learn__ library: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "We will create one encoder the features matrix and another for the dependent variable matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 44.0, 72000.0],\n",
       "       [2, 27.0, 48000.0],\n",
       "       [1, 30.0, 54000.0],\n",
       "       [2, 38.0, 61000.0],\n",
       "       [1, 40.0, 63777.77777777778],\n",
       "       [0, 35.0, 58000.0],\n",
       "       [2, 38.77777777777778, 52000.0],\n",
       "       [0, 48.0, 79000.0],\n",
       "       [1, 50.0, 83000.0],\n",
       "       [0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Creating the object\n",
    "labelencoder_X = LabelEncoder()\n",
    "\n",
    "# Fitting the first column (Country) with our encoder\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is wrong in our encoding. If you encode countries with numbers like 0, 1 or 2; it seems like you have an relation between values (Spain code is 1 so it's better than France code which is 0) and the machine will think that. We could use this encoding if we were using categories like sizes (small, medium, large)\n",
    "\n",
    "To prevent the machine to think in the wrong way, we are going to use the __Dummy encoding__. Instead of having one column we are going to have many columns (the number is equal to the number of categories)\n",
    "\n",
    "So for example if we are in the France column it's going to be 1 if the country is France and 0 if not.\n",
    "\n",
    "<img src=\"img/Dummy_encoding_detail.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "\n",
    "To do this we are going to use the __OneHotEncoder__ class : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  44,  72000],\n",
       "       [ 0,  0,  1,  27,  48000],\n",
       "       [ 0,  1,  0,  30,  54000],\n",
       "       [ 0,  0,  1,  38,  61000],\n",
       "       [ 0,  1,  0,  40,  63778],\n",
       "       [ 1,  0,  0,  35,  58000],\n",
       "       [ 0,  0,  1,  39,  52000],\n",
       "       [ 1,  0,  0,  48,  79000],\n",
       "       [ 0,  1,  0,  50,  83000],\n",
       "       [ 1,  0,  0,  37,  67000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Creating the OneHotEncoder object - You can put there the index of the columns\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "\n",
    "# Fitting the matrix with the OneHotEncoder\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "\n",
    "# Formatting numpy to only show integer and not float\n",
    "np.set_printoptions(formatter={'float': '{: 0.0f}'.format})\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the Dependent Variable\n",
    "labelencoder_y = LabelEncoder()\n",
    "Y = labelencoder_y.fit_transform(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset into test and train sets\n",
    "\n",
    "A machine learning model is going make predictions using a subset of the dataset by finding some \"correlations\" in the data. \n",
    "\n",
    "That's why we have to split the dataset into two sets : one which will allow to train de model and an other to test predictions.\n",
    "\n",
    "To do this we are going to use the __train_test_split__ method of the __cross validation__ library : http://scikit-learn.org/0.16/modules/generated/sklearn.cross_validation.train_test_split.html\n",
    "\n",
    "__Parameters__:\n",
    "* arrays : sequence of arrays or scipy.sparse matrices with same shape[0]\n",
    "\n",
    "    _Python lists or tuples occurring in arrays are converted to 1D numpy arrays._\n",
    "\n",
    "* test_size : float, int, or None (default is None)\n",
    "\n",
    "    _If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. If train size is also None, test size is set to 0.25._\n",
    "\n",
    "* train_size : float, int, or None (default is None)\n",
    "\n",
    "    _If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size._\n",
    "\n",
    "* random_state : int or RandomState\n",
    "\n",
    "We are going to create 4 subsets : 2 for the matrix of features (X_train, X_test) and another for the maxtrix of the dependant variables (Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yanni iyeze\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Creating subset , 20% of the data is going to the test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.777778</td>\n",
       "      <td>63777.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.693793</td>\n",
       "      <td>12265.579662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>48000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>54000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>61000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>72000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>83000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Age        Salary\n",
       "count   9.000000      9.000000\n",
       "mean   38.777778  63777.777778\n",
       "std     7.693793  12265.579662\n",
       "min    27.000000  48000.000000\n",
       "25%    35.000000  54000.000000\n",
       "50%    38.000000  61000.000000\n",
       "75%    44.000000  72000.000000\n",
       "max    50.000000  83000.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look to our dataset, we see that Age and Salary don't have the same scale (values from 27 to 50 from Age and 48000 to 83000 for Salary) and that can cause issues in a marchine learning model.\n",
    "\n",
    "Because lot of machine learning model use the Euclidian distance (distance between two points)\n",
    "\n",
    "<img src=\"img/Euclidian_distance.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "\n",
    "if we take our example, the euclidian distance will be dominated by the salary and age won't \"exist\"\n",
    "\n",
    "There are many ways to scale the data:\n",
    "* __Standardisation__: for each observation or feature you withdraw the mean og all the values and divide by the standard deviation\n",
    "\n",
    "* __Normalization__: we substract the observation X by the min of all future values and you divide it by the diff between the max and min of the future values.\n",
    "\n",
    "<img src=\"img/Feature_Scaling.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "\n",
    "To do this we are going to use the __StandardScaler__ class of the scikit learn preprocessing library:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  3, -1,  0,  0],\n",
       "       [ 1, -0, -1, -0,  0],\n",
       "       [-1, -0,  1, -2, -2],\n",
       "       [-1, -0,  1,  0, -1],\n",
       "       [ 1, -0, -1,  2,  2],\n",
       "       [-1, -0,  1, -0, -0],\n",
       "       [ 1, -0, -1,  1,  1],\n",
       "       [ 1, -0, -1, -1, -0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Creating Scaler object for the matrix of features\n",
    "scaler_X = StandardScaler()\n",
    "\n",
    "# For the training set you have to fit and transform it\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "\n",
    "# For the test set we don't have to fit it because its already fitted to the training set\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# It's a not a obligation to Scale the Dummy variables - As u want, it depends of the context\n",
    "# We are not going to scale Y \n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
