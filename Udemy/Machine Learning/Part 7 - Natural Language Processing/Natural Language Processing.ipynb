{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing \n",
    "\n",
    "__NLP uses:__\n",
    "\n",
    "- Sentiment analysis\n",
    "- Predict genre of book\n",
    "- Question answering\n",
    "- Machine translator or speech recognition\n",
    "\n",
    "__Librairies:__ Spacy, NLTK ...\n",
    "\n",
    "__Bag of Words__:\n",
    "\n",
    "Very popular NLP model used to preprocess the texts to classify before fitting the classification algorithms on the observations containing the texts.\n",
    "\n",
    "It involves two things:\n",
    "\n",
    "- A vocabulary of known words\n",
    "- A measure of the presence of known words\n",
    "\n",
    "In this section, we will understand and learn how to:\n",
    "\n",
    "- Cleans text to prepare them for machine learning models\n",
    "- Create a Bag of words model\n",
    "- Apply machine learning models onto this bag of worlds model.\n",
    "\n",
    "## Practical Example\n",
    "\n",
    "The dataset contains reviews of a restaurant and the goal is to separate the good and bad reviews.\n",
    "\n",
    "The tsv format is use because the tab separator in our context use is the best to use (a comma will create other columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset - Quoting for ingnoring double quotes\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning to Bag of words\n",
    "\n",
    "The goal is to only get the relevant words and avoid punctuation, numbers, capitals or stop words and also apply stemming to get the root of a word and avoid different version of a word.\n",
    "\n",
    "At the end we will apply the tokenization process to create our bag of words by splitting the text to a matrix of words (in columns).\n",
    "\n",
    "---\n",
    "\n",
    "## __Step 1:__ Only keeping the letters and remove punctuation and numbers\n",
    "\n",
    "Using regex and sub method using a regular expression `^a-zA-Z]`\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "# adding the space to replace the removed characters\n",
    "review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : Wow... Loved this place.\n",
      "After : Wow    Loved this place \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][0])\n",
    "print(\"Before : {}\".format(dataset['Review'][0]))\n",
    "print(\"After : {}\".format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## __Step 2:__ Putting all the letters to lowercase\n",
    "```python\n",
    "#To lowercase\n",
    "review = review.lower()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putting all the letters to lowercase...\n",
      "\n",
      "Before : Wow    Loved this place \n",
      "After : wow    loved this place \n"
     ]
    }
   ],
   "source": [
    "review_lower = review.lower()\n",
    "\n",
    "print(\"Putting all the letters to lowercase...\\n\")\n",
    "print(\"Before : {}\".format(re.sub('[^a-zA-Z]', ' ', dataset['Review'][0])))\n",
    "print(\"After : {}\".format(review_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## __Step 3:__ Remove the non significant words (stopwords)\n",
    "\n",
    "If we are using the first line we see that words like `this` is not really usefull for machine learning algo.\n",
    "\n",
    "To remove the stopwords we are going to use yhe __nltk__ library and its stopwords list.\n",
    "\n",
    "For each review we will split the text in several words and check if each of the words are in the stopwords list.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "\n",
    "# Importing and dowload the list of useless words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#split the text\n",
    "review = review.split()\n",
    "\n",
    "# removing stop words - set function is used find faster stop words matches\n",
    "review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting text...\n",
      "\n",
      "Before : wow    loved this place \n",
      "After : ['Wow', 'Loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "review_splitted = review.split()\n",
    "\n",
    "print(\"Splitting text...\\n\")\n",
    "\n",
    "print(\"Before : {}\".format(re.sub('[^a-zA-Z]', ' ', dataset['Review'][0]).lower()))\n",
    "print(\"After : {}\".format(review_splitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords...\n",
      "\n",
      "Before : ['Wow', 'Loved', 'this', 'place']\n",
      "After : ['Wow', 'Loved', 'place']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "review_no_stop_words = [word for word in review_splitted if not word in set(stopwords.words('english'))]\n",
    "print(\"Removing stopwords...\\n\")\n",
    "print(\"Before : {}\".format(review_splitted))\n",
    "print(\"After : {}\".format(review_no_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## __Step 4:__ Stemming\n",
    "\n",
    "To get the root of a word to avoid the different versions.\n",
    "\n",
    "```python\n",
    "# importing PorterStemmer class\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create an object of the Porter stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Applying steamer to our list of words\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "\n",
    "# convert our list to text\n",
    "review = ' '.join(review)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming ...\n",
      "\n",
      "Before : ['Wow', 'Loved', 'place']\n",
      "After : ['wow', 'love', 'place']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "review_stemmed_no_stop_words = [ps.stem(word) for word in review_no_stop_words if not word in set(stopwords.words('english'))]\n",
    "print(\"Stemming ...\\n\")\n",
    "print(\"Before : {}\".format(review_no_stop_words))\n",
    "print(\"After : {}\".format(review_stemmed_no_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating text on each line ...\n",
      "\n",
      "Before : ['wow', 'love', 'place']\n",
      "After : wow love place\n"
     ]
    }
   ],
   "source": [
    "review_text_stemmed_no_stop_words = ' '.join(review_stemmed_no_stop_words)\n",
    "\n",
    "print(\"Recreating text on each line ...\\n\")\n",
    "print(\"Before : {}\".format(review_stemmed_no_stop_words))\n",
    "print(\"After : {}\".format(review_text_stemmed_no_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Full code for cleaning process (for each review)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yanni-benoit-\n",
      "[nltk_data]     iyeze/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Before cleaning... ************* \n",
      " \n",
      "Wow... Loved this place. \n",
      "\n",
      "Crust is not good. \n",
      "\n",
      "Not tasty and the texture was just nasty. \n",
      "\n",
      "Stopped by during the late May bank holiday off Rick Steve recommendation and loved it. \n",
      "\n",
      "The selection on the menu was great and so were the prices. \n",
      "\n",
      "Now I am getting angry and I want my damn pho. \n",
      "\n",
      "Honeslty it didn't taste THAT fresh.) \n",
      "\n",
      "The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer. \n",
      "\n",
      "The fries were great too. \n",
      "\n",
      "A great touch.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************* After cleaning ... ************* \n",
      "\n",
      "wow love place \n",
      "\n",
      "crust good \n",
      "\n",
      "tasti textur nasti \n",
      "\n",
      "stop late may bank holiday rick steve recommend love \n",
      "\n",
      "select menu great price \n",
      "\n",
      "get angri want damn pho \n",
      "\n",
      "honeslti tast fresh \n",
      "\n",
      "potato like rubber could tell made ahead time kept warmer \n",
      "\n",
      "fri great \n",
      "\n",
      "great touch\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    # appending clean review to our list of common words \n",
    "    corpus.append(review)\n",
    "    \n",
    "print(\"************* Before cleaning... ************* \\n \")\n",
    "print(' \\n\\n'.join(dataset['Review'][:10].tolist()))\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"************* After cleaning ... ************* \\n\")\n",
    "print(' \\n\\n'.join(corpus[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Creating the bag of words model\n",
    "\n",
    "The bag of words model is used after creating the corpus and to create it you have to take all disctinct words and create on column for each word (__tokenization__). \n",
    "That will create a matrix of the reviews and word in column, if a line contain a word there will be a 1 and 0 instead.\n",
    "\n",
    "We need to create this model to use a machine learning model because the ml model will be trained on the reviews and help it to understand the correlation (review <-> word) for the classification : __we use independent variable (word) to predict dependent variable (good or bad).__\n",
    "\n",
    "We use the [__CountVectorizer__](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) method of the __sklearn library__ to reproduce the tokenization process\n",
    "\n",
    "We already cleaned the text before so we don't need to use every parameters. \n",
    "That's should be more useful if you wand to apply more cleaning steps for complicated text like scrapped text.\n",
    "\n",
    "\n",
    "```python\n",
    "# importing the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Creating CountVectorizer object - MaxFeatures is used to remove non relevant words (we have 1500 column).\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "\n",
    "# Fitting our corpus using tokenisation - Matrix of features or independent variable\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "#Selecting dependent variable from dataset - Reviews likes\n",
    "y = dataset.iloc[:, 1].values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Matrix of feature... ************* \n",
      " \n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "************* Independant variable... ************* \n",
      " \n",
      "[1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "print(\"************* Matrix of feature... ************* \\n \")\n",
    "print(X[:5])\n",
    "\n",
    "print(\"************* Independant variable... ************* \\n \")\n",
    "\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Applying a Machine Learning Model\n",
    "\n",
    "The most common models used for NLP are Naive Bayes, Decision Trees and Random forest models.\n",
    "\n",
    "For our example we will try the Naive Bayes model.\n",
    "\n",
    "```python\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Prediction... ************* \n",
      " \n",
      "[1 1 1 0 0]\n",
      "************* Results... ************* \n",
      " \n",
      "************* Confusion Matrix... ************* \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[55, 42],\n",
       "       [12, 91]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"************* Prediction... ************* \\n \")\n",
    "print(y_pred[:5])\n",
    "\n",
    "# Comparing Predictions with Test set\n",
    "print(\"************* Results... ************* \\n \")\n",
    "\n",
    "results = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred})\n",
    "l = []\n",
    "for index, row in results.iterrows():\n",
    "    if row['y_test'] == row['y_pred']:\n",
    "        result = 'Yes'\n",
    "    else:\n",
    "        result = 'No'\n",
    "    l.append(result)\n",
    "results['is ok ?'] = l\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"************* Confusion Matrix... ************* \\n \")\n",
    "cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 55 + 91 good results so 73%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework challenge \n",
    "```\n",
    "Hello students,\n",
    "\n",
    "congratulations for having completed Part 7 - Natural Language Processing.\n",
    "\n",
    "If you are up for some practical activities, here is a little challenge:\n",
    "\n",
    "1. Run the other classification models we made in Part 3 - Classification, other than the one we used in the last tutorial.\n",
    "\n",
    "2. Evaluate the performance of each of these models. Try to beat the Accuracy obtained in the tutorial. But remember, Accuracy is not enough, so you should also look at other performance metrics like Precision (measuring exactness), Recall (measuring completeness) and the F1 Score (compromise between Precision and Recall). Please find below these metrics formulas (TP = # True Positives, TN = # True Negatives, FP = # False Positives, FN = # False Negatives):\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 Score = 2 * Precision * Recall / (Precision + Recall)\n",
    "\n",
    "3. Try even other classification models that we haven't covered in Part 3 - Classification. Good ones for NLP include:\n",
    "\n",
    "    CART\n",
    "    C5.0\n",
    "    Maximum Entropy\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
