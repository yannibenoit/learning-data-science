{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "<img src=\"img/dl_simple.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<img src=\"img/dl.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "## Plan of attack\n",
    "\n",
    "- Neuron\n",
    "- Activation function\n",
    "- How do Neural Networks works ?\n",
    "- How do Neural Networks learn ?\n",
    "- Gradient Descent\n",
    "- Stochastic Gradient Descent\n",
    "- Backpropagation\n",
    "\n",
    "---\n",
    "\n",
    "## Neuron\n",
    "\n",
    "<img src=\"img/neuron.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "A neuron alone is useless, it needs to be connected with others thanks to the dendrite and axone. The connections is called the synapse.\n",
    "\n",
    "So the neuron received input signals and send an output signal.\n",
    "\n",
    "<img src=\"img/neuron_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "The input signals are more specifically input values and are collected by the brain thanks to sensors. The signal passe through synaps.\n",
    "\n",
    "Input values are independent variable we need to standardize or normalize.\n",
    "\n",
    "<img src=\"img/neuron_3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Output value can be continuous, binary or categorical (different values).\n",
    "\n",
    "<img src=\"img/neuron_4.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "The synapse have weight which can be adjusted (using Gradient Descent and Backpropagation) to make the network more efficient.\n",
    "\n",
    "<img src=\"img/neuron_5.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "Inside the neuron, all the input values and the weight are aggregated using a sum formula and will apply a function called __activation function__ to the aggregation.\n",
    "\n",
    "<img src=\"img/neuron_6.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "This process is replicated in each neuron of each layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Activation function\n",
    "\n",
    "The are different types of activation function :\n",
    "\n",
    "- Threshold Function\n",
    "\n",
    "<img src=\"img/af_threshold.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "This simple function looks like a yes or no function.\n",
    "\n",
    "- Sigmoid function\n",
    "\n",
    "<img src=\"img/af_sigmoid.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "More complex function which is use in logistic regression and seems smoother. More useful for the ouput layer if you want to predict a probability.\n",
    "\n",
    "- Rectifier function\n",
    "\n",
    "<img src=\"img/af_rectifier.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Most popular activation function in artificial neural networks.\n",
    "\n",
    "- Hyperbolic Tangent function \n",
    "\n",
    "<img src=\"img/af_hyperbolic.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Similar to the SigmoÃ¯d function but start with y negative values.\n",
    "\n",
    " ### To go further \n",
    "\n",
    "[=> Activation functions](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf) \n",
    "\n",
    "Let's take our previous example which is getting as an input binary variables. In this case, it make more sense to use threshold or sigmoid activation function.\n",
    "\n",
    "<img src=\"img/af_example.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "If we take another example using an hidden layer. In this case we should use a Rectifier function for neurons in the hidden layer and a sigmoid for the output layer.\n",
    "\n",
    "<img src=\"img/af_example_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "---\n",
    "\n",
    "## How do neural networks work ?\n",
    "\n",
    "Let's take an example with a already trained neural network which will extract properties of the picture below.\n",
    "\n",
    "<img src=\"img/image_example.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "We have input parameters like Aera, bedrooms, distance to city, age ...\n",
    "The varaibles will be weighted up and sent using a synapse to the output layer in order to calculate the price of the house (We could use different type of activation function : machine lerning algorithm, sigmoid ...).\n",
    "\n",
    "<img src=\"img/image_example_simple_nn.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "To get more accurracy and efficiency using our neural network, we need to use neuros in the hidden layer to get this \"extra-power\".\n",
    "Each synapse have a weight and some can be equal to 1 for example or 0: not all synapses get a weight bigger than 0, so some inputs will be more important than others.\n",
    "\n",
    "For example, a neuron can specifically try to get only data about the area and distance from the city.\n",
    "\n",
    "<img src=\"img/image_example_simple_nn_n1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "An other neuron can be specialised into other parameters : Aera, Bedrooms, Age because it could take as an hypothesis : \" maybe in this aera big properties are old\" , \"the population demography has change and they want newer property\". The neuron will combine these 3 parameters.\n",
    "\n",
    "<img src=\"img/image_example_simple_nn_n2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "An other neuron can just take in account on parameter like Age because the older properties are less valuable but a really old one can be historical.\n",
    "\n",
    "\n",
    "<img src=\"img/image_example_simple_nn_n3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "And the a neuron can pick up all parameters.\n",
    "\n",
    "<img src=\"img/image_example_simple_nn_n4.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "So this hidden layer allows us to increase the flexibility of the neural network and using different parameters combination at each neuron look to specific things.\n",
    "\n",
    "<img src=\"img/image_example_simple_nn_n5.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "All together neurons of the hidden layer will improve the accuracy of the prediction of our neural network.\n",
    "\n",
    "---\n",
    "\n",
    "## How do neural networks learn ?\n",
    "\n",
    "You have to methods to help a machine to learn and give results or do actions:\n",
    "\n",
    "- hard coding : code everything your program need to do \n",
    "- neural networks : give just the input and waiting for the output by letting the nn working.\n",
    "\n",
    "Let's take an example with a simple neurol network using one layer.\n",
    "We need to adjust the the ouput value to seperate this value (^y - predicted value) from the actual value (y - reality).\n",
    "\n",
    "<img src=\"img/learn_example_simple_nn.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "So when input values are sent into our neural network, we get a predicted output value and we need to compare this value with the actual value. We see that values are different if we plot them. Then we need to use a cost function (there are many types) to properly compare the actual and output value and return the error of the prediction.\n",
    "\n",
    "<img src=\"img/learn_example_simple_nn_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "After calculating the cost value, this information will be sent back to the neural network in order the update the weights. \n",
    "\n",
    "<img src=\"img/learn_example_simple_nn_3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Our goal is to minimize our cost function by updating the weights.\n",
    "\n",
    "So we will repeat these actions to get at the end the lowest cost value. \n",
    "\n",
    "<img src=\"img/learn_example_simple_nn_4.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "If we trained our network using many rows at the same time (the example below just using one line to train the nn).\n",
    "\n",
    "Each time we will train our neural network using all rows in the dataset we will make an __epoch__.\n",
    "\n",
    "We will send each row as input values and compare the actual and predicted value by calculating the global cost value using a cost function\n",
    "\n",
    "<img src=\"img/learn_example_simple_nn_5_1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"img/learn_example_simple_nn_5_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"img/learn_example_simple_nn_5_3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"img/learn_example_simple_nn_5_4.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"img/learn_example_simple_nn_5_5.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "And when we finish our loop we go back to update the weights of our neural network (all the rows share the weight).\n",
    "\n",
    "\n",
    "<img src=\"img/learn_example_simple_nn_5_6.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "And we are repeating the process until we minimize the cost value.\n",
    "This process use to adjust weights, it's called __back propagation__\n",
    "\n",
    "### To go futher:  \n",
    "\n",
    "[=> A list of cost functions used in neural networks, alongside applications](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient descent \n",
    "\n",
    "___How the weight are ajusted to minimize cost function return ?___\n",
    "\n",
    "If we take a simple example and stay focused on one neuron in a layer which is getting a weighted input value and return a predicted value using an activation function. We will compare this predicted value with the actual by calculating the return of a cost function.\n",
    "\n",
    "\n",
    "<img src=\"img/gradient_descent_example_0.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "How can we minimize this return ?\n",
    "\n",
    "One apporach is to plot the weights using the predicted value and cost return and choosing the best weight by taking the lower one.\n",
    "\n",
    "<img src=\"img/gradient_descent_example_1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "This approach would work if you had one weight to adjust but with many of them we faced the __curse of dimensionality__.\n",
    "\n",
    "<img src=\"img/gradient_descent_example_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Using this kind of approach will occur a very huge number combinaisons of weights which will too much and long to process.\n",
    "\n",
    "So we have to use another method called __Gradient Descent__.\n",
    "\n",
    "This method consists of watching the angle and slope for a cost return, if we plot weight using the cost and predicted value. That's will make move our points to the left or the right until we mimimize it.\n",
    "\n",
    "<img src=\"img/gradient_descent_example_3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"img/gradient_descent_example_4.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"img/gradient_descent_example_5.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"img/gradient_descent_example_6.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"img/gradient_descent_example_7.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "__Example of 2D Gradient descent__\n",
    "\n",
    "<img src=\"img/gradient_descent_example_2D.png\" width=\"400\" height=\"200\">\n",
    "<br>\n",
    "\n",
    "__Example of 3D Gradient descent__\n",
    "\n",
    "<img src=\"img/gradient_descent_example_3D.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "---\n",
    "## Stochastic Gradient descent \n",
    "\n",
    "If we use another cost function, we are not going to get a convex line after ploting the cost return and predicted value. So using a normal gradient descent method in this case won't help us to find the optimal weights to minimize the cost\n",
    "\n",
    "<img src=\"img/stochastic_gradient_descent_example_0.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "The gradient descent take all rows to be applied and the stochastic gradient descent will take row one by one so we will adjust the weight several time.\n",
    "\n",
    "<img src=\"img/stochastic_gradient_descent_example_1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "This method is lighter and faster than the Batch gradient descent because it will process randomly row one by one.\n",
    "\n",
    "There is another one call the __Mini Batch Gradient descent method__ which is a mixed of the Stochastic and normal one.\n",
    "\n",
    "### To go futher:  \n",
    "\n",
    "[=> A Neural Network in 13 lines of Python (Part 2 - Gradient Descent)](https://iamtrask.github.io/2015/07/27/python-network-part2/)\n",
    "\n",
    "[=> Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "<img src=\"img/forward_propagation.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Forward propagation is the method which propagate weighted signal from the neurons input layer to the output layer and then we calculated the errors using the predicted value.\n",
    "\n",
    "<br>\n",
    "\n",
    "Backpropagation is when we send back the errors to adjust the weights\n",
    "\n",
    "<img src=\"img/back_propagation.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "The backpropagation is an advanced algorithm which allow us __all weights at the same time__.\n",
    "\n",
    "### To go futher:  \n",
    "\n",
    "[=> Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "\n",
    "## Training the ANN with a Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "<img src=\"img/Stochastic_Gradient_Descent.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "## Practical example\n",
    "\n",
    "___\n",
    "\n",
    "### Installation Tips\n",
    "\n",
    "#### Installing Theano\n",
    "\n",
    "Theano is an open source library for fast computations for GPU (graphics, more powerful) and CPU (your computer).\n",
    "\n",
    "```bash\n",
    "pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "```\n",
    "\n",
    "#### Installing Tensorflow\n",
    "\n",
    "Tensorflow is another open source library developped by Google Brain used to develop neural network from scratch.\n",
    "\n",
    "[Install Tensorflow from the website](https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html)\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "\n",
    "```\n",
    "\n",
    "#### Installing Keras\n",
    "\n",
    "Open source library to build neural network using few lines or codes.\n",
    "\n",
    "```bash\n",
    "pip install --upgrade keras\n",
    "```\n",
    "___\n",
    "\n",
    "## Dataset \n",
    "\n",
    "We are going to use a bank dataset with contains data of their customers. The company recently noticed a strange churn rate (when people leave the company) and want to understand the root case using different measures/variables : Surname, Credit score, Gender, Estimated Salary etc.\n",
    "\n",
    "The company start to take the measure for a sample of the customers (about 10%) 6 month ago and now the column Exited says if the customer is still in the bank or not.\n",
    "\n",
    "Our goal is to find geographic segmentation with the customers with the highest risk to leave.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "1. Importing librairies and the csv Dataset\n",
    "\n",
    "2. Separate the features (dependent variables - X - from Geography -> Estimated Salary) and the independant variable (Y - Exited) by building matrix.\n",
    "\n",
    "3. Encode categorical data (Geography and Gender) and create dummy variables because Geography contains 3 categories and Gender 2.\n",
    "\n",
    "4. Splitting the dataset into the Training set and Test set\n",
    "\n",
    "5. Apply feature scaling because there will be a lot of computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Encoding Geography **********\n",
      "[[619 0 'Female' ... 1 1 101348.88]\n",
      " [608 2 'Female' ... 0 1 112542.58]\n",
      " [502 0 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 0 'Female' ... 0 1 42085.58]\n",
      " [772 1 'Male' ... 1 0 92888.52]\n",
      " [792 0 'Female' ... 1 0 38190.78]]\n",
      "************ Encoding Gender **********\n",
      "[[619 0 0 ... 1 1 101348.88]\n",
      " [608 2 0 ... 0 1 112542.58]\n",
      " [502 0 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 0 0 ... 0 1 42085.58]\n",
      " [772 1 1 ... 1 0 92888.52]\n",
      " [792 0 0 ... 1 0 38190.78]]\n",
      "************ Create dummy variables **********\n",
      "[[0.0000000e+00 0.0000000e+00 6.1900000e+02 ... 1.0000000e+00\n",
      "  1.0000000e+00 1.0134888e+05]\n",
      " [0.0000000e+00 1.0000000e+00 6.0800000e+02 ... 0.0000000e+00\n",
      "  1.0000000e+00 1.1254258e+05]\n",
      " [0.0000000e+00 0.0000000e+00 5.0200000e+02 ... 1.0000000e+00\n",
      "  0.0000000e+00 1.1393157e+05]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 7.0900000e+02 ... 0.0000000e+00\n",
      "  1.0000000e+00 4.2085580e+04]\n",
      " [1.0000000e+00 0.0000000e+00 7.7200000e+02 ... 1.0000000e+00\n",
      "  0.0000000e+00 9.2888520e+04]\n",
      " [0.0000000e+00 0.0000000e+00 7.9200000e+02 ... 1.0000000e+00\n",
      "  0.0000000e+00 3.8190780e+04]]\n",
      "************ Training set and Test set **********\n",
      "************ Feature Scaling **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Geography variable\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "\n",
    "print(\"************ Encoding Geography **********\")\n",
    "print(X)\n",
    "\n",
    "# Gender variable\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "\n",
    "print(\"************ Encoding Gender **********\")\n",
    "print(X)\n",
    "\n",
    "# Create Dummy variables for Geography\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "\n",
    "print(\"************ Create dummy variables **********\")\n",
    "print(X)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"************ Training set and Test set **********\")\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print(\"************ Feature Scaling **********\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building ANN\n",
    "\n",
    "<img src=\"img/Stochastic_Gradient_Descent.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "### 1 - Importing Librairies\n",
    "\n",
    "```python\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "\n",
    "# Initialize the model\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Build Layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Tensorflow backend \n",
    "from tensorflow.keras import backend\n",
    "\n",
    "```\n",
    "### 2 - Initialising Artificial Neural Networks\n",
    "\n",
    "```python\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "```\n",
    "### 3 - Adding the Input Layer and first hidden layer\n",
    "\n",
    "__parameters:__\n",
    "\n",
    "- `input_dim` : The number of nodes in the input layer is the number of the independant variables -> __11__.\n",
    "\n",
    "- `output_dim` : The number of nodes in the ouput layer is the average number of the independant variables of both layers:  11  / 2 -> __6__.\n",
    "\n",
    "- `activation` : activation function , we are using the __rectifier (relu)__\n",
    "\n",
    "- `init`: the __uniform__ value make sure that weights are uniformly and randomly distributed.\n",
    "\n",
    "\n",
    "```python\n",
    "# Adding the input layer and the first hidden layer \n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "```\n",
    "### 4 - Adding the second hidden layer\n",
    "\n",
    "__parameters:__\n",
    "\n",
    "- `output_dim` : The number of nodes in the ouput layer is the average number of the independant variables of both layers:  11  / 2 -> __6__.\n",
    "\n",
    "- `activation` : activation function , we are using the __rectifier (relu)__\n",
    "\n",
    "- `init`: the __uniform__ value make sure that weights are uniformly and randomly distributed.\n",
    "\n",
    "```python\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "```\n",
    "\n",
    "### 5 - Adding the output layer\n",
    "\n",
    "__parameters:__\n",
    "\n",
    "- `output_dim` : we only want a single result for our output layer -> __1__.\n",
    "\n",
    "- `activation` : in this case,  we are using the __sigmoid__ function because we want a probability as a result.\n",
    "PS: use the `soft_max` function if you have more than 2 categories.\n",
    "\n",
    "- `init`: the __uniform__ value make sure that weights are uniformly and randomly distributed.\n",
    "\n",
    "```python\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "```\n",
    "\n",
    "### 6 - Compiling the ANN\n",
    "\n",
    "__parameters:__\n",
    "\n",
    "- `optimizer`: the optimizer is the algorithm you want to use to find the optimal set of weights in the neural networks. We are going to use the __Adam__ algo which is a __efficient type of Sochastic Gradient descent algorithm__\n",
    "\n",
    "- `loss`: the loss is the loss function used with the Sochastic Grandient descent algorithm : logarithmic loss function -> __binary_crossentropy__ because we have a binary outcome (we shoud use categorical_crossentropy if we had more results for the outcome).\n",
    "\n",
    "\n",
    "<img src=\"img/loss_function.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "- `metrics`: criterium used by the algorithm to improve the model performance : __accuracy__.\n",
    "\n",
    "\n",
    "```python\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "```\n",
    "\n",
    "### 7 - Fitting the ANN to the Training set\n",
    "\n",
    "__parameters:__\n",
    "\n",
    "- `batch_size`: Number of rows\n",
    "\n",
    "- `nb_epoch`: Number of epochs\n",
    "\n",
    "```python\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.4903 - acc: 0.7955\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4265 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4220 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4188 - acc: 0.8189\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.4167 - acc: 0.8259\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4148 - acc: 0.8287\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4132 - acc: 0.8296\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4119 - acc: 0.8297\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4108 - acc: 0.8321\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4096 - acc: 0.8337\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4091 - acc: 0.8336\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4083 - acc: 0.8325\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4078 - acc: 0.8331\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4070 - acc: 0.8345\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4067 - acc: 0.8347\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4064 - acc: 0.8360\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4050 - acc: 0.8334\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4055 - acc: 0.8347\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4050 - acc: 0.8330\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4044 - acc: 0.8347\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4042 - acc: 0.8339\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4042 - acc: 0.8339\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4036 - acc: 0.8349\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4034 - acc: 0.8346\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4029 - acc: 0.8340\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4028 - acc: 0.8364\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4027 - acc: 0.8345\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4028 - acc: 0.8346\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4024 - acc: 0.8340\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4025 - acc: 0.8359\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4020 - acc: 0.8365\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4022 - acc: 0.8342\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4017 - acc: 0.8350\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4023 - acc: 0.8339\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4015 - acc: 0.8336\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4017 - acc: 0.8355\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 0.4016 - acc: 0.8350\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.4015 - acc: 0.8357\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.4016 - acc: 0.8360\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.4013 - acc: 0.8352\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4016 - acc: 0.8347\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4013 - acc: 0.8349\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4009 - acc: 0.8361\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4011 - acc: 0.8331\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4009 - acc: 0.8351\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4010 - acc: 0.8346\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4014 - acc: 0.8341\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.4005 - acc: 0.8346\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.4011 - acc: 0.8354\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.4010 - acc: 0.8345\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.4007 - acc: 0.8354\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.4008 - acc: 0.8354\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 0.4008 - acc: 0.8347\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4005 - acc: 0.8342\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4013 - acc: 0.8344\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4007 - acc: 0.8352\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4009 - acc: 0.8347\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.4007 - acc: 0.8337\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.4005 - acc: 0.8339\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.4007 - acc: 0.8337\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4009 - acc: 0.8357\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4007 - acc: 0.8346\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4007 - acc: 0.8347\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4001 - acc: 0.8346\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4008 - acc: 0.8347\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4006 - acc: 0.8352\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4006 - acc: 0.8354\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4003 - acc: 0.8346\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4003 - acc: 0.8350\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4006 - acc: 0.8354\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4006 - acc: 0.8355\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4003 - acc: 0.8350\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4001 - acc: 0.8350\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4001 - acc: 0.8351\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4006 - acc: 0.8364\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4001 - acc: 0.8356\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4005 - acc: 0.8351\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4003 - acc: 0.8352\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4003 - acc: 0.8354\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4002 - acc: 0.8352\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 0.4006 - acc: 0.8345\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4002 - acc: 0.8337\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.3998 - acc: 0.8354\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4004 - acc: 0.8344\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4004 - acc: 0.8356\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4003 - acc: 0.8349\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4001 - acc: 0.8366\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4002 - acc: 0.8356\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4003 - acc: 0.8356\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.3998 - acc: 0.8357\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4000 - acc: 0.8351\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4004 - acc: 0.8354\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.3999 - acc: 0.8347\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4003 - acc: 0.8352\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4003 - acc: 0.8346\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4001 - acc: 0.8360\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.4002 - acc: 0.8347\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4003 - acc: 0.8356\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.3999 - acc: 0.8351\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4001 - acc: 0.8357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124b7df28>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "\n",
    "# Initialize the model\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Build Layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Tensorflow backend \n",
    "from tensorflow.keras import backend\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer \n",
    "# 11 input node because of the 11 independent variable\n",
    "# Use rectifier activation function\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "# Use rectifier activation function\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "# Use sigmoÃ¯d activation function\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17421031]\n",
      " [0.3468189 ]\n",
      " [0.13791767]\n",
      " ...\n",
      " [0.2021403 ]\n",
      " [0.14789769]\n",
      " [0.11822695]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "# Need to convert probabilities to Boolean values \n",
    "# We need take the standard 50% threshold \n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1540,   55],\n",
       "       [ 265,  140]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 1540 + 140 correct predictions -> 83% !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
